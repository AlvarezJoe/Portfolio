{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9911a96d",
   "metadata": {},
   "source": [
    "<h1><center>CAP 4631C. Spring 2023</center></h1>\n",
    "<h1><center>Assignment 5</center></h1>\n",
    "<h3><center>Introduction</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74baf52",
   "metadata": {},
   "source": [
    "- This assignment is about Random Forest Regression.\n",
    "\n",
    "- This assignment uses the Hitters data from assignment 4. Therefore, one more time, the outcome variable to __predict is the log of Salary__, where Salary is the player’s salary in the 1987 season. Notice that the column called “Salary” in this dataset actually records the log of the Salary.\n",
    "\n",
    "- Use the train_test_split() method and use 80% of the players to form the training dataset.\n",
    "\n",
    "- Use all the predictors when constructing the forests with the exception of “League”, “Division”, and “New League”. (this bullet point was added on Thursday 02-23-23)\n",
    "\n",
    "__Question 1__: Apply Random Forest using approach 1 (the theory-based approach) and version 2 (where you need to select the best values for both the number of features and the number of trees)\n",
    "\n",
    "__1 a)__ Report the mean squared error of the chosen forest evaluated on the test dataset you created earlier (i.e., the 20% players that you left out earlier to serve as a test dataset).\n",
    "\n",
    "__Note__: Do not forget that this question asks you to apply the second version of approach 1 that we practiced in class (i.e., you need to tune both the number of features and the number of trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92d90e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from abess import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1337efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv\n",
    "hitters_df= pd.read_csv('Hitters_ML_HW4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65038371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 263 entries, 0 to 262\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Hits       263 non-null    int64  \n",
      " 1   HmRun      263 non-null    int64  \n",
      " 2   Runs       263 non-null    int64  \n",
      " 3   RBI        263 non-null    int64  \n",
      " 4   Years      263 non-null    int64  \n",
      " 5   CAtBat     263 non-null    int64  \n",
      " 6   CHits      263 non-null    int64  \n",
      " 7   CHmRun     263 non-null    int64  \n",
      " 8   CRuns      263 non-null    int64  \n",
      " 9   CRBI       263 non-null    int64  \n",
      " 10  League     263 non-null    object \n",
      " 11  Division   263 non-null    object \n",
      " 12  NewLeague  263 non-null    object \n",
      " 13  Salary     263 non-null    float64\n",
      "dtypes: float64(1), int64(10), object(3)\n",
      "memory usage: 28.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#quick eda\n",
    "hitters_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f7bff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>NewLeague</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>N</td>\n",
       "      <td>6.163315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>A</td>\n",
       "      <td>6.173786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>6.214608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>4.516339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>A</td>\n",
       "      <td>6.620073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hits  HmRun  Runs  RBI  Years  CAtBat  CHits  CHmRun  CRuns  CRBI League  \\\n",
       "0    81      7    24   38     14    3449    835      69    321   414      N   \n",
       "1   130     18    66   72      3    1624    457      63    224   266      A   \n",
       "2   141     20    65   78     11    5628   1575     225    828   838      N   \n",
       "3    87     10    39   42      2     396    101      12     48    46      N   \n",
       "4   169      4    74   51     11    4408   1133      19    501   336      A   \n",
       "\n",
       "  Division NewLeague    Salary  \n",
       "0        W         N  6.163315  \n",
       "1        W         A  6.173786  \n",
       "2        E         N  6.214608  \n",
       "3        E         N  4.516339  \n",
       "4        W         A  6.620073  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick eda cont.\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cfc7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split creation\n",
    "x_train, x_test, y_train, y_test= train_test_split(\n",
    "    hitters_df.iloc[:,:-4],hitters_df['Salary'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9398af",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    "\n",
    "_(Theory Based)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c8d8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider 50 to 500 trees in steps of 10\n",
    "number_of_trees=np.arange(50,501,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85f8909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "3.1622776601683795\n"
     ]
    }
   ],
   "source": [
    "#print feature count and square root of feature count\n",
    "print(x_train.shape[1])\n",
    "print(np.sqrt(x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16e8415a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 46)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of trees with X features\n",
    "number_of_features=np.arange(3,6)\n",
    "mse_scores_rf_oob_matrix= np.empty((number_of_features.size, number_of_trees.size))\n",
    "mse_scores_rf_oob_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64770ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding best MSE for combination of trees and features\n",
    "r=0\n",
    "for i in number_of_features:\n",
    "    c=0\n",
    "    for j in number_of_trees:\n",
    "        rf_loop= RandomForestRegressor(n_estimators = j, oob_score= True, max_features=i, random_state=1)\n",
    "        rf_loop.fit(x_train, y_train)\n",
    "        mse_scores_rf_oob_matrix[r,c]= mean_squared_error(y_train, rf_loop.oob_prediction_)\n",
    "        c=c+1\n",
    "    r= r+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3079c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print number of features\n",
    "number_of_features[np.where(mse_scores_rf_oob_matrix == np.min(mse_scores_rf_oob_matrix))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89c197a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([460])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print number of trees\n",
    "number_of_trees[np.where(mse_scores_rf_oob_matrix == np.min(mse_scores_rf_oob_matrix))[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18878d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create random forest with number of features and trees\n",
    "rf= RandomForestRegressor(n_estimators= 460, max_features=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df4c9b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=3, n_estimators=460, random_state=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit training set to random forest\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e619a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15953913430856442"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MSE\n",
    "mean_squared_error( y_test,rf.predict (x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cd517c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39942350244892255"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSE\n",
    "mean_squared_error( y_test,rf.predict (x_test), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70811c27",
   "metadata": {},
   "source": [
    "___Here we have our MSE and RMSE (my personal favorite) using the theory based approach for non-random forests. They are .159 and .399 respectively. From a purely objective approach these seem to be very good values. I'd personnaly like to compare the RMSE to the average of the log of salary.___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028cfbf7",
   "metadata": {},
   "source": [
    "__Question 2__: Apply Random Forest using approach 2 (the practice-based approach)\n",
    "\n",
    "__2 a)__ Report the mean squared error of the chosen forest evaluated on the test dataset you created earlier (i.e., the 20% players that you left out earlier to serve as a test dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddcf701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create parameters for gridsearch\n",
    "param_grid_rf = {  'n_estimators': np.arange(50,501,20), 'max_features': np.arange(3,6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "326d9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create grid with params and cross validation of 5\n",
    "gridSearch_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d5c154b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid={'max_features': array([3, 4, 5]),\n",
       "                         'n_estimators': array([ 50,  70,  90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290,\n",
       "       310, 330, 350, 370, 390, 410, 430, 450, 470, 490])},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit training data to grid\n",
    "gridSearch_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01c48f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  {'max_features': 3, 'n_estimators': 270}\n"
     ]
    }
   ],
   "source": [
    "#find best params (random = 0)\n",
    "print('Parameters: ', gridSearch_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c6ba14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define forest with best params\n",
    "rf2= RandomForestRegressor(n_estimators= 270 , max_features=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4a9ebe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=3, n_estimators=50, random_state=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train forest\n",
    "rf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd6e7f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18296560494884415"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MSE\n",
    "mean_squared_error( y_test,rf2.predict (x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2247ff65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4277447895051957"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSE\n",
    "mean_squared_error( y_test,rf2.predict (x_test), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab772b",
   "metadata": {},
   "source": [
    "___Here we have our MSE and RMSE using the practice based approach for non-random forests. They are .183 and .427 respectively. We can see that using the theory based approach is lending better results (MSE = .159).___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26c11b",
   "metadata": {},
   "source": [
    "__2 b)__ Report the three most important predictors that form the trees part of this forest. Show how you selected the three most important predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "144930fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAtBat    0.213392\n",
       "CRuns     0.199416\n",
       "CHits     0.182947\n",
       "CRBI      0.107660\n",
       "CHmRun    0.066294\n",
       "RBI       0.058874\n",
       "Hits      0.057284\n",
       "Years     0.046167\n",
       "HmRun     0.034094\n",
       "Runs      0.033872\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature importance with sorting\n",
    "feature_imp = pd.Series(rf2.feature_importances_, hitters_df.iloc[:,:-4].columns [rf2.feature_importances_!=0])\n",
    "feature_imp.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce453c",
   "metadata": {},
   "source": [
    "___Our three most impotant features are CAtBat, CRuns, and CHits for predicting log of salary using our second non-random forest (theory based).___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860eefc",
   "metadata": {},
   "source": [
    "__Question 3__: Select the best Forest from those you got in questions 1 and 2. __Justify your selection__. Then, compare the chosen forest to the best single tree that you got in assignment 4. State which one is best (the best forest or the best single tree?). __Justify your answer__.\n",
    "\n",
    "__Note__: You do not have to repeat the steps you did in assignment 4 to get the best single tree. Just mention what your best single tree was from assignment 4 and why. Then, compare it to the best forest.\n",
    "\n",
    "__Note__: Do not forget that this question asks you to justify on two occasions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef0c2a",
   "metadata": {},
   "source": [
    "___Comparing our MSE we clearly see that our first random forest ourperformed our second. Our first MSE was .159 and our second MSE was .183. This clearly indicates that of the two possible models we should use the theory based approach for non-random forests.___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45731516",
   "metadata": {},
   "source": [
    "___In the previous assignment our best performing model was a post pruned tree (MSE equaling .179). When comparing our first non-random forest to our previous model from assignment 4, we find that our non-random forest utlizing a theory based approach performed much better than any of our existing models (MSE equaling .159 for our first non-random forest). That means that for our specific dataset, prediciting the log of salary is best done by using a non-random forest that utlilizes a theory based approach.___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
