{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcc0f05",
   "metadata": {},
   "source": [
    "# <center> Getting Data Ready for MLOps Monitoring <center/>\n",
    "\n",
    "## <center> Setting up housing data for model tracking and drift detection <center/>\n",
    "    \n",
    "## <center> By Joemichael Alvarez <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd420c",
   "metadata": {},
   "source": [
    "***Data Set:*** `California Housing Dataset`: \n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)\n",
    "\n",
    "***Data Set Information:***\n",
    "\n",
    "- **Number of instances 20,640**\n",
    "\n",
    "- **Number of Attributes 9**\n",
    "\n",
    "- **Attribute breakdown 8 quantitative input variables, and 1 quantitative output variable**\n",
    "\n",
    "- **Missing Attribute Values None**\n",
    "\n",
    "***Attribute Information:***\n",
    "\n",
    "Given are the variable name, variable type, the measurement unit and a brief description. The median house value is our regression target. \n",
    "\n",
    "- **MedInc** -- quantitative -- median income in block group -- Input Variable\n",
    "- **HouseAge** -- quantitative -- median house age in block group -- Input Variable\n",
    "- **AveRooms** -- quantitative -- average number of rooms per household -- Input Variable\n",
    "- **AveBedrms** -- quantitative -- average number of bedrooms per household -- Input Variable\n",
    "- **Population** -- quantitative -- block group population -- Input Variable\n",
    "- **AveOccup** -- quantitative -- average number of household members -- Input Variable\n",
    "- **Latitude** -- quantitative -- block group latitude -- Input Variable\n",
    "- **Longitude** -- quantitative -- block group longitude -- Input Variable\n",
    "- **MedHouseVal** -- quantitative -- median house value in hundreds of thousands of dollars -- Output Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45a7c5",
   "metadata": {},
   "source": [
    "# What We're Building\n",
    "\n",
    "Before diving into MLOps concepts, it's important to understand what we're building. This project demonstrates a complete MLOps pipeline using housing data to simulate real-world model monitoring scenarios.\n",
    "\n",
    "**We seek to create a production-ready ML system that can detect when model performance degrades due to data drift, concept drift, or other factors that occur in real deployed systems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef1c10",
   "metadata": {},
   "source": [
    "# Setting Up the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7fb3d",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea767ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "#our custom modules\n",
    "from data_loader import DataLoader\n",
    "\n",
    "#quality of life\n",
    "np.random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e52905",
   "metadata": {},
   "source": [
    "## Loading Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced97f8",
   "metadata": {},
   "source": [
    "### First Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78146327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "#load housing data\n",
    "print(\"Loading California housing dataset...\")\n",
    "df = data_loader.load_housing_data()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2033cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display first few records vertically and rounded\n",
    "df.head(10).round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic statistics for our columns\n",
    "df.describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05092d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how i like to display the shape of my df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6be64",
   "metadata": {},
   "source": [
    "The California housing dataset is clean with no missing values. All features are quantitative and properly formatted. For MLOps simulation, we need to add temporal dimensions to simulate real-world data streaming scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a628b",
   "metadata": {},
   "source": [
    "# <center>Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3dd96f",
   "metadata": {},
   "source": [
    "## Looking at Each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac08725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram univariate\n",
    "for col in df.columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(data=df, x=col, multiple=\"dodge\")\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f737c6",
   "metadata": {},
   "source": [
    "Most features show reasonable distributions. Income and house values show some right skew which is typical for economic data. Geographic features (latitude/longitude) show clustering patterns reflecting California's population centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551713a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How Features Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085bd32",
   "metadata": {},
   "source": [
    "Lets sort some of our more important correlations with house values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e055e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create correlations with MedHouseVal\n",
    "correlations = df.drop(\"MedHouseVal\", axis=1).apply(lambda x: x.corr(df.MedHouseVal, method='spearman'))\n",
    "\n",
    "#sort the correlations in descending order\n",
    "correlations.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a30c1",
   "metadata": {},
   "source": [
    "## Adding Time to the Mix\n",
    "\n",
    "For MLOps monitoring, we need to add timestamps to simulate real-world data streams. The original dataset lacks temporal information, but production systems receive data over time.\n",
    "\n",
    "___Domain Knowledge:___ Housing markets exhibit temporal patterns due to economic cycles, seasonal trends, and policy changes. Adding timestamps allows us to simulate drift detection scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add timestamps to simulate data streaming\n",
    "print(\"Adding timestamps to simulate real-world data streaming...\")\n",
    "df_with_time = data_loader.add_timestamps(df, start_date=\"2023-01-01\")\n",
    "\n",
    "print(f\"Dataset with timestamps shape: {df_with_time.shape}\")\n",
    "print(\"\\nNew temporal columns:\")\n",
    "print(df_with_time[['timestamp', 'date']].head())\n",
    "\n",
    "#check time range coverage\n",
    "print(f\"\\nTime range:\")\n",
    "print(f\"Start: {df_with_time['timestamp'].min()}\")\n",
    "print(f\"End: {df_with_time['timestamp'].max()}\")\n",
    "print(f\"Duration: {df_with_time['timestamp'].max() - df_with_time['timestamp'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how our house values look over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Just grab every 1000th point so the plot doesn't get too crowded\n",
    "sample_df = df_with_time.iloc[::1000].copy()\n",
    "\n",
    "plt.scatter(sample_df['timestamp'], sample_df['MedHouseVal'], \n",
    "           alpha=0.6, s=20, color='navy')\n",
    "plt.title('House Prices Over Time (Sample of Our Data)')\n",
    "plt.xlabel('When the Data Came In')\n",
    "plt.ylabel('Median House Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ed406",
   "metadata": {},
   "source": [
    "## Splitting Data the Right Way\n",
    "\n",
    "MLOps requires temporal splits rather than random splits to simulate realistic model deployment scenarios where models are trained on historical data and tested on future data.\n",
    "\n",
    "___MLOps Insight:___ Time-based splits prevent data leakage and reflect real-world model performance where future data may differ from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporal split at mid-year point\n",
    "split_date = \"2023-07-01\"  \n",
    "\n",
    "train_data, val_data = data_loader.split_by_time(df_with_time, split_date)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"\\nTraining period: {train_data['timestamp'].min()} to {train_data['timestamp'].max()}\")\n",
    "print(f\"Validation period: {val_data['timestamp'].min()} to {val_data['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to save everything so we can use it later\n",
    "print(\"Saving all our processed data...\")\n",
    "\n",
    "# Make sure we have the right folders\n",
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Save the full dataset with our new timestamps\n",
    "data_loader.save_processed_data(df_with_time, \"housing_with_timestamps.csv\")\n",
    "\n",
    "# Save our train/validation splits\n",
    "data_loader.save_processed_data(train_data, \"train_data.csv\")\n",
    "data_loader.save_processed_data(val_data, \"validation_data.csv\")\n",
    "\n",
    "# Create a baseline dataset - we'll use this as our \"reference\" for drift detection\n",
    "baseline_data = train_data[train_data['timestamp'] < '2023-02-01'].copy()\n",
    "data_loader.save_processed_data(baseline_data, \"baseline_reference.csv\")\n",
    "\n",
    "print(f\"✅ Saved baseline reference: {baseline_data.shape[0]} records from early 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb435d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily chunks for our monitoring demo\n",
    "print(\"Breaking data into daily batches for monitoring simulation...\")\n",
    "\n",
    "# Use some of our validation data and limit it so we don't go crazy with files\n",
    "data_loader.create_daily_batches(val_data.head(10000))  \n",
    "\n",
    "print(\"✅ Got our daily batches ready for monitoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5a1cf",
   "metadata": {},
   "source": [
    "## Quick Quality Check\n",
    "\n",
    "Perform basic data quality validation before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a45743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if our data is clean and ready to go\n",
    "def perform_data_quality_checks(df, dataset_name):\n",
    "    print(f\"\\n=== Quality Check: {dataset_name} ===\")\n",
    "    \n",
    "    # Any missing values?\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Missing values:\")\n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"✅ No missing values - we're good!\")\n",
    "    else:\n",
    "        print(f\"{missing_values[missing_values > 0]}\")\n",
    "    \n",
    "    # Duplicate rows?\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates == 0:\n",
    "        print(\"✅ No duplicate rows found\")\n",
    "    else:\n",
    "        print(f\"⚠️ Found {duplicates} duplicate rows\")\n",
    "    \n",
    "    # What types are we working with?\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Check for outliers using the IQR method (pretty standard approach)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    outlier_counts = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col not in ['timestamp']:  # Skip timestamp since it's not really a \"feature\"\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "            outlier_counts[col] = outliers\n",
    "    \n",
    "    print(f\"\\nOutliers (using IQR method):\")\n",
    "    for col, count in outlier_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} outliers ({count/len(df)*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"  {col}: No outliers ✅\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Run our checks on all the datasets\n",
    "perform_data_quality_checks(train_data, \"Training Data\")\n",
    "perform_data_quality_checks(val_data, \"Validation Data\")\n",
    "perform_data_quality_checks(baseline_data, \"Baseline Reference Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d92117",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "### What We Got Done\n",
    "\n",
    "**Data Processing:**\n",
    "- ✅ Loaded California housing dataset (20,640 instances, 9 attributes)\n",
    "- ✅ Added temporal dimension for MLOps simulation\n",
    "- ✅ Implemented temporal data splitting\n",
    "- ✅ Created baseline reference dataset for drift detection\n",
    "- ✅ Generated daily batches for monitoring simulation\n",
    "- ✅ Performed data quality validation\n",
    "\n",
    "**Output Files:**\n",
    "- `../data/processed/housing_with_timestamps.csv` - Full dataset with temporal features\n",
    "- `../data/processed/train_data.csv` - Training split (first 6 months)\n",
    "- `../data/processed/validation_data.csv` - Validation split (last 6 months)\n",
    "- `../data/processed/baseline_reference.csv` - Reference data for drift monitoring\n",
    "- `../data/processed/day_*.csv` - Daily batches for monitoring\n",
    "\n",
    "**Next Steps:**\n",
    "- 📊 **Notebook 02**: Model training with MLflow experiment tracking\n",
    "- 🔄 **Notebook 03**: Data drift simulation and detection\n",
    "- 📈 **Notebook 04**: Production monitoring with Evidently AI\n",
    "\n",
    "The preprocessed data is ready for model training and MLOps pipeline implementation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
